---
title: "P8105_HW6_yz4717"
author: "Yang Zhao"
date: "2023-12-02"
output: github_document
---

```{r import_needed_packages, message=FALSE}
library(tidyverse)
library(modelr)
```

# Problem 1

## data cleaning
```{r}
homicide= 
  read_csv("data/homicide-data.csv") |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    solve_not = case_match(
      disposition,
      "Closed without arrest" ~ 0,
      "Open/No arrest"        ~ 0,
      "Closed by arrest"      ~ 1) 
    ) |>
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |>
  filter(victim_race %in% c("White", "Black")) |> 
  mutate(
    victim_age = as.numeric(victim_age)
  ) |>
  select(city_state, solve_not, victim_age, victim_sex, victim_race)
```


## Baltimore glm function

```{r}
baltimore_glm = 
  filter(homicide, city_state == "Baltimore, MD") |> 
  glm(solve_not ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)
```

## estimate and confidence interval of the adjusted odds ratio

```{r}
baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

## Estimated ORs and CIs for each city.

```{r}
all_cities = 
  homicide |>
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(solve_not ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

all_cities |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

## Making plot

```{r}
all_cities |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

In most cities, male victims have smaller odds than female victims as the odds ratio is smaller than 1. In roughly half of these cities, confidence intervals are narrow and do not contain 1, suggesting a significant difference in resolution rates by sex after adjustment for victim age and race. 


# Problem 2

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

```{r building_model_q2}
weather_q2_df =
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    modeling = map(strap, \(data) lm(tmax ~ tmin + prcp, data = data)),
    mapping = map(modeling, broom::glance),
    result = map(modeling, broom::tidy)) |>
  select(.id, result, mapping) |> 
  unnest(result, mapping) |>
  select(.id, r.squared, term, estimate) |>
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |>
  mutate(
    log_data = log(tmin * prcp)
  ) |>
  select(.id, r.squared, log_data)

head(weather_q2_df)
```

I use the `bootstrap` to perform resampling.And I conduct the linear regression modeling, and various data transformations on the given data set. And I use `mutate` to calculate the needed value in the question and `unnest` the result as well.

Then, I would like to plot the r_squared distribution.

```{r density_plot_r^2}

weather_q2_df |>
  ggplot(aes(x = r.squared)) +
  geom_density()+
  labs(x = "R_Squared",
       y = "Density",
       title = "The Density Plot of R_squared")

```

As can be seen from the above graph, the distribution is approximately normal with symmetric tails and the median of 0.917. 

After that, I plot the distribution of ($log(\hat{\beta_1} * \hat{\beta_2})$)

```{r density_plot_log}

weather_q2_df |>
  ggplot(aes(x = log_data)) +
  geom_density()+
  labs(x = "Log",
       y = "Density",
       title = "The Density Plot of Log Data")

```

From above graph, we can clearly see that it is a left-skewed distribution with a long tail on the left. And the center of this distribution is between -6 and -5.

* Confidence interval of ${r^2}$ and $log(\hat{\beta_1} * \hat{\beta_2})$

* Here comes the 95% CI of r_squared.

```{r quantile_r_squared}

quantile(
  pull(weather_q2_df,
       r.squared),
  probs = c(.025, .975)) |> 
  knitr::kable(col.names = c("95% of CI"))

```

* Here comes the 95% CI of $log(\hat{\beta_1} * \hat{\beta_2})$ excluding `NULL` data.

```{r quantile_log}

quantile(
  pull(weather_q2_df,
       log_data),
  probs = c(.025, .975),
  na.rm = TRUE) |> 
  knitr::kable(col.names = c("95% of CI"))

```

## Problem 3


```{r import_data_q3}

birthweight_df =
  read_csv('data/birthweight.csv', na = c("", "NA", "Unknown"))

birthweight_df =
  birthweight_df |>
  mutate(
    babysex = case_match(
             babysex,
             1 ~ "male",
             2 ~ "female"),
    frace = case_match(
             frace,
             1 ~ "White", 
             2 ~ "Black", 
             3 ~ "Asian", 
             4 ~ "Puerto Rican", 
             8 ~ "Other", 
             9 ~ "Unknown"),
    malform = case_match(
             malform,
             0 ~ "absent",
             1 ~ "present"),
    mrace = case_match(
             mrace,
             1 ~ "White", 
             2 ~ "Black", 
             3 ~ "Asian", 
             4 ~ "Puerto Rican", 
             8 ~ "Other"),
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace))

head(birthweight_df)

```

First of all, I import the data into the environment and use the function `mutate` and `case_match` to update the value in the data. Then, I convert these variables into `factors`.

```{r building_linear_model}

lm_q3 = lm(bwt ~ .,
           data = birthweight_df)

summary(lm_q3)

lm_q3_original = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks +mheight + momage + parity + ppwt + smoken, data = birthweight_df)

summary(lm_q3_original)

```

At first, I have used all the variables in the data set as predictors. Then, I reviewed the summary result and check the p-values of all predictors. As we known, to make the adjusted r-squared higher, I need to drop the insignificant variables in the original model. Repeat this step, until all predictors have a p-value less than 0.05.


```{r prediction_residual}

birthweight_df |>
  add_predictions(lm_q3) |>
  add_residuals(lm_q3) |>
  ggplot(aes(x = pred,
             y = resid)) +
  geom_point(color = "black",
             alpha = .5) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "blue") +
  labs(x = "Fitted Values",
       y = "Residuals",
       title = "The Plot of Birthweight Model")

```

Most points gathered in the region of 2000-4000 fitted values and -1000-1000 residuals. These dots are gathered into a cluster. A small number of points are distributed far away from the cluster in the periphery of the cluster.


## Length at birth and gestational age

```{r q3_model_1}

set.seed(123)

lm_q3_1 = 
  lm(bwt ~ blength + gaweeks,
              data = birthweight_df)

summary(lm_q3_1)

```

## Head circumference, length, sex, and all interactions

```{r q3_model_2}

lm_q3_2  =  
  lm(bwt ~ babysex + blength + bhead + + bhead * blength + bhead * babysex + blength * babysex, 
     data = birthweight_df)

summary(lm_q3_2)

```


## Comparison

### Original Model

```{r original_model}

cv_model = crossv_mc(birthweight_df, 100)

errors_original_df = map2_dbl(
  pull(cv_model,train),
  pull(cv_model,test), 
  ~{
  model1_fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + momage + parity + ppwt + smoken,
                  data = birthweight_df)
  rmse(model1_fit, .y)
  })

error_original = mean(errors_original_df)

```

## Model 1 & Model 2

```{r model1_model2}

# model 1
errors_model1_df = 
  map2_dbl(
  pull(cv_model,train),
  pull(cv_model,test), 
  ~{
  model1_fit = lm(bwt ~ blength + gaweeks, data = .x)
  rmse(model1_fit, .y)
  })

error_model1 = mean(errors_model1_df)

# model 2
errors_model2_df = 
  map2_dbl(
  pull(cv_model,train),
  pull(cv_model,test),  
  ~{
  model2_fit = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex, data = .x)
  rmse(model2_fit, .y)
})

error_model2 = mean(errors_model2_df)

```

We can use `crossv_mc` to split the data into k exclusive partitions and uses each partition for a test-training split. Then, I built three different models as requested in the question and computed their error.

```{r comparison}

tibble(
  "original" = error_original,
  "model_1" = error_model1,
  "model_2" = error_model2
) |> 
  knitr::kable()
```

From the lessons we already have, we can know that Error can be used to compare different models. According to the given result, we can clearly see the optimized original model have the lowest Error while the other model perform badly.
